{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ee38d6",
   "metadata": {},
   "source": [
    "### import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcd3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold, GridSearchCV,\n",
    "    KFold, \n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold, \n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f305a47",
   "metadata": {},
   "source": [
    "### setup csv file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b019b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(__name__).resolve().parent.parent\n",
    "BASE_FILE_PATH = BASE_DIR / 'tabular-playground-series-nov-2022'\n",
    "SUBMISSION_FILES_PATH = BASE_FILE_PATH / 'submission_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fead08",
   "metadata": {},
   "source": [
    "### code transcription(TPS Nov 2022 | EDA + Hybrid Stacking 🚀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755cac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED\n",
    "\n",
    "seed = 2484\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a2c3c",
   "metadata": {},
   "source": [
    "#### Files\n",
    "- submission_files/ - a folder containing binary model predictions\n",
    "- train_labels.csv - the ground truth labels for the first half of the rows in the submission files\n",
    "- sample_submission.csv - a sample submission file in the correct format, only containing the row ids for the second - - half of each file in the submissions folder; your task is to blend together submissions that achieve the improvements in the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb4e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(BASE_FILE_PATH / 'sample_submission.csv', index_col='id')\n",
    "\n",
    "labels = pd.read_csv(BASE_FILE_PATH / 'train_labels.csv', index_col='id')\n",
    "\n",
    "# the ids of the submission rows (useful later)\n",
    "sub_ids = submission.index\n",
    "\n",
    "# the ids of the labeled rows (useful later)\n",
    "gt_ids = labels.index \n",
    "\n",
    "# list of files in the submission folder\n",
    "subs = sorted(os.listdir(SUBMISSION_FILES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5b4e1",
   "metadata": {},
   "source": [
    "#### Labels distribution\n",
    "The ground truth for these rows are provided in the file train_labels.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd115ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b160912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "labels_names = [f\"{p:.2f}%\" for p in labels.value_counts() / labels.value_counts().sum() * 100]\n",
    "labels.value_counts().plot(kind='pie', ax=ax[0], labels=labels_names, colors=(\"r\", \"b\"), ylabel=\"label\")\n",
    "labels.value_counts().plot(kind='bar', ax=ax[1], color=(\"r\", \"b\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c377319",
   "metadata": {},
   "source": [
    "#### Submissions files\n",
    "Each file name in the submissions folder corresponds to the logloss score of the the first half of the prediction rows (20k rows) against the ground truth labels in that file. This is effectively the \"training\" set.\n",
    "\n",
    "(트레이닝 세트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f036d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = pd.read_csv(SUBMISSION_FILES_PATH / subs[0], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13e57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222863195.csv  log_loss: 0.6222863195\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "log_loss \n",
    "모델 성능 평가 시 사용 가능한 지표\n",
    "분류(Classification) 모델 평가시 사용합니다.\n",
    "값이 작을수록 좋은 모델\n",
    "'''\n",
    "\n",
    "score = log_loss(labels, s0.loc[gt_ids])\n",
    "\n",
    "# Notice the score of the labeled rows matches the file name\n",
    "print(subs[0],' log_loss:', f'{score:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f01629d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb7e10",
   "metadata": {},
   "source": [
    "#### Loading all submission files\n",
    "we are going to load all submission files into a dataframe with final shape 40k x 5k. Each Submission file will go into a column of the dataframe\n",
    "\n",
    "We also calculate the ROC-AUC for each submission file in order to use this metric later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c392462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [06:09, 13.54it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_orig = np.zeros((s0.shape[0], len(subs))) # (5000, 40000)\n",
    "roc_auc_scores = dict()\n",
    "\n",
    "for i, name in tqdm(enumerate(subs)):\n",
    "    \n",
    "    # submission_files에 파일들을 하나씩 읽어와서 X_train_orig에 첫번째 row에 저장\n",
    "    sub = pd.read_csv(SUBMISSION_FILES_PATH / name, index_col='id')\n",
    "    X_train_orig[:, i] = sub.pred.values\n",
    "    \n",
    "    # roc_auc_score 정확도 평가\n",
    "    # ROC Curve의 면적, AUC의 값이 1일 수록 좋은 모델이라 평가\n",
    "    # (실제 정답 리스트(정수), 1일 확률이 담긴 리스트(실수)) 이렇게 연산한 결과가 1에 가까울 정도로 높은 수치가 나옴\n",
    "    # labels.label 20k개, X_train_orig에서 i columns의 0~20k까지의 row 값\n",
    "    auc_score = roc_auc_score(labels.label[0:20000], X_train_orig[0:20000, i])\n",
    "    \n",
    "    # 연산된 수치값을 roc_auc_scores에 저장\n",
    "    roc_auc_scores[name] = auc_score\n",
    "    \n",
    "# X_train_orig으로 만들어진 array?로 DataFrame 생성\n",
    "X_train_orig = pd.DataFrame(X_train_orig, columns=subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba8c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab690f2",
   "metadata": {},
   "source": [
    "#### Removing Strange submission files\n",
    "we are going to remove strange submission files with values >1 or <0 As result we remove 108 submission files\n",
    "\n",
    "Instead of removing strange submission files we try to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa48476",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train_orig = X_train_orig.loc[:, X_train_orig.max()<=1]\n",
    "X_train_orig = X_train_orig.loc[:, X_train_orig.min()>=0]\n",
    "\n",
    "numpy.clip(array, min, max)\n",
    "array 내의 element들에 대해서\n",
    "min 값 보다 작은 값들을 min값으로 바꿔주고\n",
    "max 값 보다 큰 값들을 max값으로 바꿔주는 함수.\n",
    "'''\n",
    "\n",
    "X_train_orig = X_train_orig.clip(0, 1)\n",
    "\n",
    "# 2 variables removed since they were low-information variables\n",
    "# 0.6933054832.csv, 0.6933472206.csv cloumns 값이 전부 동일\n",
    "DROP_LOW_INFO_FEATURES = ['0.6933054832.csv', '0.6933472206.csv']\n",
    "\n",
    "# 제거후 X_train_orig에 적용 \n",
    "X_train_orig = X_train_orig[list(set(X_train_orig.columns) - set(DROP_LOW_INFO_FEATURES))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f14d6b",
   "metadata": {},
   "source": [
    "#### Calibration of Train set (Train set 보정)\n",
    "Our train set is composed of several models' predictions output, so it's a good idea to calibrate their output before training a blended model with these  \n",
    "(현재 Train set는 여러 모델의 예측 값으로 구성되어 있어 혼합 모델을 training 전 보정이 필요)\n",
    "\n",
    "ref. https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/363778"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacdb76",
   "metadata": {},
   "source": [
    "##### sklearn CalibrationDisplay\n",
    "\n",
    "교정 곡선(신뢰도 다이어그램이라고도 함) 시각화\n",
    "실제 레이블과 예측 확률을 사용하여 보정(교졍) 곡선을 그립니다.\n",
    "\n",
    "---\n",
    "\n",
    "**Methods**\n",
    "\n",
    "**from_estimator(estimator, X, y, *[, n_bins, ...]):** Plot calibration curve using a binary classifier and data.\n",
    "\n",
    "**from_predictions(y_true, y_prob, *[, ...]):** Plot calibration curve using true labels and predicted probabilities.\n",
    "\n",
    "**plot(*[, ax, name, ref_line]):** Plot visualization.\n",
    "\n",
    "---\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "\n",
    "**'X':** {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "    Input values.\n",
    "    \n",
    "**'y':** array-like of shape (n_samples,)\n",
    "    Binary target values.\n",
    "    \n",
    "**'n_bins':** int, default=5\n",
    "    Number of bins to discretize the [0, 1] interval into when calculating the calibration curve. \n",
    "    A bigger number requires more data.\n",
    "    \n",
    "**'strategy':** {‘uniform’, ‘quantile’}, default=’uniform’\n",
    "    Strategy used to define the widths of the bins.\n",
    "    \n",
    "    - 'uniform': The bins have identical widths.\n",
    "    - 'quantile': The bins have the same number of samples and depend on predicted probabilities.\n",
    "        \n",
    "**'pos_label':** str or int, default=None\n",
    "    The positive class when computing the calibration curve. \n",
    "    By default, estimators.classes_[1] is considered as the positive class.\n",
    "    \n",
    "**'name':** str, default=None\n",
    "    Name for labeling curve. If None, the name of the estimator is used.\n",
    "    \n",
    "**'ref_line':** bool, default=True\n",
    "    If True, plots a reference line representing a perfectly calibrated classifier.\n",
    "    \n",
    "**'ax':** matplotlib axes, default=None\n",
    "    Axes object to plot on. If None, a new figure and axes is created.\n",
    "    \n",
    "---\n",
    "\n",
    "ref: https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibrationDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "298aa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of an uncalibrated model prediction on train set\n",
    "\n",
    "X_train_orig[:20000]['0.6222863195.csv']\n",
    "X_train_orig[:]['0.6223807245.csv'].plot(kind='hist', bins=100)\n",
    "    \n",
    "CalibrationDisplay.from_predictions(\n",
    "    labels.label[0:20000], \n",
    "    X_train_orig[0:20000]['0.6223807245.csv'], \n",
    "    n_bins=20,\n",
    "    strategy='quantile', \n",
    "    color='#ffd700')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9e036",
   "metadata": {},
   "source": [
    "##### sklearn sotonicRegression 등회귀\n",
    "\n",
    "■ 등회귀는 언제 사용되나?\n",
    "등회귀는 통계적 추론에 사용된다. 예를 들어, x의 순서에 따라 y값이 증가할 것으로 예상되는 데이터 셋의 경우, 등회귀식을 적용할 수 있다. \n",
    "분류(classification) 문제에서는 모델의 예측값을 보정(calibration)할 때도 사용된다. \n",
    "\n",
    "---\n",
    "\n",
    "**Methods**\n",
    "\n",
    "**fit(X, y[, sample_weight]):** Fit the model using X, y as training data.\n",
    "\n",
    "**fit_transform(X[, y]):** Fit to data, then transform it.\n",
    "\n",
    "**get_feature_names_out([input_features]):** Get output feature names for transformation.\n",
    "\n",
    "**get_params([deep]):** Get parameters for this estimator.\n",
    "\n",
    "**predict(T):** Predict new data by linear interpolation.\n",
    "\n",
    "**score(X, y[, sample_weight]):** Return the coefficient of determination of the prediction.\n",
    "\n",
    "**set_params(**params):** Set the parameters of this estimator.\n",
    "\n",
    "**transform(T):** Transform new data by linear interpolation.\n",
    "\n",
    "---\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "**y_minfloat, default=None:**\n",
    "Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.\n",
    "\n",
    "**y_maxfloat, default=None:**\n",
    "Upper bound on the highest predicted value (the maximum may still be lower). If not set, defaults to +inf.\n",
    "\n",
    "**increasingbool or ‘auto’, default=True:**\n",
    "Determines whether the predictions should be constrained to increase or decrease with X. ‘auto’ will decide based on the Spearman correlation estimate’s sign.\n",
    "\n",
    "**out_of_bounds{‘nan’, ‘clip’, ‘raise’}, default=’nan’:**\n",
    "Handles how X values outside of the training domain are handled during prediction.\n",
    "\n",
    "- ‘nan’, predictions will be NaN.\n",
    "\n",
    "- ‘clip’, predictions will be set to the value corresponding to the nearest train interval endpoint.\n",
    "\n",
    "- ‘raise’, a ValueError is raised.\n",
    "\n",
    "---\n",
    "\n",
    "ref: https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db7f1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set calibration \n",
    "# ref. https://www.kaggle.com/competitions/tabular-playground-series-nov-2022/discussion/363778\n",
    "\n",
    "roc_auc_scores_calibrated = {}\n",
    "X_train_calibrated = X_train_orig.copy()\n",
    "\n",
    "for i, c in tqdm(enumerate(X_train_orig.columns)):\n",
    "    \n",
    "    # 길이가 40k인 list 생성\n",
    "    x_model_calibration = np.zeros(40000)\n",
    "    \n",
    "    # 등회귀 모델 생성\n",
    "    model_calibration = IsotonicRegression(out_of_bounds='clip')\n",
    "    \n",
    "    # train 데이터를 fit_transform()을 하여 범위를 맞추고 학습후? 변환 및 저장 \n",
    "    # clip으로 min=0.001, max=0.999로 변환\n",
    "    x_model_calibration[:20000] = model_calibration.fit_transform(\n",
    "        X_train_orig[:20000][c], labels.label).clip(0.001, 0.999)\n",
    "    \n",
    "    # 나머지 부분도 변환 후 저장\n",
    "    # clip으로 min=0.001, max=0.999로 변환\n",
    "    x_model_calibration[20000:] = model_calibration.transform(\n",
    "        X_train_orig[20000:][c]).clip(0.001, 0.999)\n",
    "    \n",
    "    # 결과 값 X_train_calibrated dict에 저장\n",
    "    X_train_calibrated[c] = x_model_calibration\n",
    "    \n",
    "    # roc_auc_score 정확도 평가\n",
    "    # ROC Curve의 면적, AUC의 값이 1일 수록 좋은 모델이라 평가\n",
    "    auc_score = roc_auc_score(labels.label[0:20000], x_model_calibration[0:20000])\n",
    "    \n",
    "    # 평가된 값을 roc_auc_scores_calibrated dict에 저장\n",
    "    roc_auc_scores_calibrated[c] = auc_score\n",
    "    \n",
    "    # 시각화\n",
    "    pd.DataFrame(X_train_calibrated[:20000][X_train_calibrated.columns[0]]).plot(\n",
    "        kind='hist', bins=100)\n",
    "    \n",
    "    # 실제 레이블과 예측 확률을 사용하여 보정(교정) 곡선 시각화.\n",
    "    CalibrationDisplay.from_predictions(\n",
    "        labels.label[0:20000], \n",
    "        X_train_calibrated[:20000][X_train_calibrated.columns[0]], \n",
    "        n_bins=20,                                \n",
    "        strategy='quantile', \n",
    "        color='#ffd700')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4771062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(roc_auc_scores_calibrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bb683",
   "metadata": {},
   "source": [
    "##### Dimensionality Reduction with ROC-AUC? Why not!\n",
    "\n",
    "---\n",
    "\n",
    "##### Idea for Feature Selection using ROC-AUC\n",
    "\n",
    "A bad classifier can be identified by the ROC curve which looks very similar, if not identical, to the diagonal of the graph, representing the performance of a purely random classifier; ROC-AUC scores close to 0.5 are considered near-random results.  \n",
    "\n",
    "---\n",
    "\n",
    "Starting from the concepts behind ROC-AUC explained above, we are going to use only submissions where the ROC-AUC is major of a threshold  \n",
    "ROC-AUC가 주요 임계값만 사용해서 제출한다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfaf2608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUC_TH = 0.787\n",
    "fig = plt.figure(figsize=(30, 5))\n",
    "fig.suptitle(\"ROC-AUC Distribution\")\n",
    "out = plt.hist(roc_auc_scores_calibrated.values(), bins=200, color=(\"b\"))\n",
    "plt.plot([AUC_TH, AUC_TH], [0, 500], linestyle='dotted', c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f66753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SELECTED_FEATURES = []\n",
    "TOP_LOG_LOSS_TH = 500\n",
    "\n",
    "\n",
    "for k, v in roc_auc_scores_calibrated.items():    \n",
    "    \n",
    "    # AUC_TH = 0.787\n",
    "    if v >= AUC_TH:\n",
    "        if k in X_train_orig.columns:\n",
    "            SELECTED_FEATURES.append(k)\n",
    "            \n",
    "print(len(SELECTED_FEATURES), SELECTED_FEATURES[:TOP_LOG_LOSS_TH])\n",
    "\n",
    "# X_train_calibrated (등회귀로 보정된 값이 저장된 dict)에 AUC_TH가 0.787 같거나 큰 key 값만 뽑아서(최대 500개) 저장\n",
    "X_train_reduced = X_train_calibrated[SELECTED_FEATURES[:TOP_LOG_LOSS_TH]]\n",
    "print(X_train_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50366212",
   "metadata": {},
   "source": [
    "##### Feature Selection with MRMR (optional)\n",
    "Maximum Relevance — Minimum Redundancy” (aka MRMR) is an algorithm used by Uber’s machine learning platform for finding the “minimal-optimal” subset of features.\n",
    "\n",
    "- Enabling FEATURE_SELECTION_ENABLED the train will be done with the features selection algorithm\n",
    "- Disabling FEATURE_SELECTION_ENABLED the train will be done with all features availables in the train set (all columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b423c8",
   "metadata": {},
   "source": [
    "##### mrmr_classif\n",
    "\n",
    "---\n",
    "\n",
    "mRMR은 filter method의 한 방법으로써, 두 가지 요소를 고려합니다. 바로, Y를 잘 예측하는 변수이면서, X들과도 중복성이 적은 변수들을 선택합니다. 이 때, Y와의 상관성과 X들과의 중복성은 일반적으로 피어슨 상관계수와 Mutual information으로 측정할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "ref: https://github.com/smazzanti/mrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a6de21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FEATURE_SELECTION_ENABLED = True\n",
    "# FEATURE SELECTION MRMR\n",
    "def feature_selection(X, y, k):\n",
    "    if not FEATURE_SELECTION_ENABLED:\n",
    "        return X.columns\n",
    "    \n",
    "    # mrmr로 Feature Selection\n",
    "    # k Parameter는 받아볼 결과 갯수?\n",
    "    out = mrmr_classif(X, y, k)\n",
    "    print(\"Features selection:\", out)\n",
    "    return out\n",
    "\n",
    "if FEATURE_SELECTION_ENABLED:\n",
    "    FEATURES_SELECTED = feature_selection(X_train_reduced[0:20000], labels.label, 50)\n",
    "    X_train_reduced = X_train_reduced[FEATURES_SELECTED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad21bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e472562",
   "metadata": {},
   "source": [
    "###### Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "**Adding Unsupervised new feature**\n",
    "\n",
    "we try to add an unsupervised new feature calculated with kmeans and others as combination of features and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9825890",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced['cluster'] = KMeans(n_clusters=5, random_state=seed).fit_predict(X_train_reduced)\n",
    "X_train_reduced['mean'] = X_train_reduced.mean(axis=1)\n",
    "\n",
    "# compose a new feature as combination of two best features\n",
    "X_train_reduced['compose_feature'] = (\n",
    "    X_train_reduced['0.6778730537.csv'] + X_train_reduced['0.6702783631.csv']) / X_train_reduced['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed457a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b763cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab373c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced['compose_feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3875f85",
   "metadata": {},
   "source": [
    "##### Blending/Stacking Model\n",
    "\n",
    "StratifiedKFold을 이용한 학습 데이터 분류 (교차검증)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48858a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "k_fold = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n",
    "y = labels\n",
    "X = X_train_reduced[0:20000]\n",
    "X_test = X_train_reduced[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61116382",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd5ded39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fdfd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd038f5",
   "metadata": {},
   "source": [
    "##### RandomSearchCV\n",
    "\n",
    "To find the best hyperparameters, we will use the RandomSearchCV. Random search is a technique more faster than GridSearchCV which calculates all possible combinations\n",
    "\n",
    "RandomSearchCV를 이용해 best hyperparameters 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c61bc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size = 0.01, shuffle=True, random_state= seed, stratify=y)\n",
    "\n",
    "X_train = X_train.reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "X_validation = X_validation.reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "y_train = y_train.reset_index()['label']\n",
    "\n",
    "y_validation = y_validation.reset_index()['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ffa6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55d5de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49b1f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f588beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "057c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_metric': 'binary_logloss', \n",
    "    'eval_set': [(X_validation, y_validation)],   \n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "param_test = {\n",
    "  'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "  'n_estimators': [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n",
    "  'num_leaves': sp_randint(6, 50, 100), \n",
    "  'min_child_samples': sp_randint(100, 500), \n",
    "  'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "  'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "  'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "  'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "  'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "  'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa5c42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_rs = LGBMClassifier(metric = 'binary_logloss', \n",
    "    random_state = seed, \n",
    "    silent = True, \n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = lgbm_rs, \n",
    "    param_distributions = param_test,     \n",
    "    scoring = 'neg_log_loss',\n",
    "    cv = k_fold,\n",
    "    refit = True,\n",
    "    random_state = seed,\n",
    "    verbose = 100, \n",
    "    n_iter = 50\n",
    ")\n",
    "\n",
    "opt_parameters = {\n",
    "    'colsample_bytree': 0.45066268483824545,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 5,\n",
    "    'min_child_samples': 285,\n",
    "    'min_child_weight': 0.01,\n",
    "    'n_estimators': 300,\n",
    "    'num_leaves': 116,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 1,\n",
    "    'subsample': 0.532329735064063\n",
    "}\n",
    "\n",
    "RS_FIT = False # enable it to use RandomSearchCV\n",
    "\n",
    "if RS_FIT:\n",
    "    random_search.fit(X, y, **fit_params)\n",
    "    opt_parameters = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d798de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f0ce2",
   "metadata": {},
   "source": [
    "##### Hybrid Model Class\n",
    "The following class permit us to encapsulate the logic of ensembling N models making a weighted average of their predictions (Soft-Voting)\n",
    "\n",
    "We are going to choose several models in order to ensemble them and try to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfe927bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleHybrid:\n",
    "   def __init__(self, models=[], weights=[]):\n",
    "       self.models = models\n",
    "       self.weights = weights\n",
    "\n",
    "   def fit(self, X, y):\n",
    "       # Train models\n",
    "       for m in self.models:\n",
    "           print(f\"Training {m}...\")\n",
    "           m.fit(X, y)\n",
    "\n",
    "   def predict_proba(self, X_test):\n",
    "       y_pred = pd.Series(np.zeros(X_test.shape[0]), index=X_test.index)\n",
    "       for i, m in enumerate(self.models):\n",
    "           y_pred += pd.Series(m.predict_proba(X_test)[:,1], index=X_test.index) * self.weights[i]\n",
    "       return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd03f390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def folds_model(debug=True):\n",
    "    # several models in order to stacking\n",
    "    lgbm = LGBMClassifier(\n",
    "        **opt_parameters, \n",
    "        objective='binary', \n",
    "        silent=True,\n",
    "        random_state=seed,\n",
    "        metric='binary_logloss'\n",
    "    )\n",
    "    \n",
    "#     # 사용하지 않는 것으로 보임\n",
    "#     lgbm_t = LGBMClassifier(\n",
    "#         objective='binary', \n",
    "#         silent=True,\n",
    "#         random_state=seed,\n",
    "#         metric='binary_logloss', \n",
    "#         bagging_fraction=0.5, \n",
    "#         bagging_freq=3, \n",
    "#         boosting_type='gbdt',\n",
    "#         class_weight=None, \n",
    "#         colsample_bytree=1.0, \n",
    "#         feature_fraction=0.7,\n",
    "#         importance_type='split', \n",
    "#         learning_rate=0.001, \n",
    "#         max_depth=-1,\n",
    "#         min_child_samples=6, \n",
    "#         min_child_weight=0.001, \n",
    "#         min_split_gain=0.9,\n",
    "#         n_estimators=140, \n",
    "#         n_jobs=-1, \n",
    "#         num_leaves=60, \n",
    "#         reg_alpha=0.4, \n",
    "#         reg_lambda=1e-07, \n",
    "#         subsample=1.0, \n",
    "#         subsample_for_bin=200000, \n",
    "#         subsample_freq=0\n",
    "#     )\n",
    "\n",
    "    cat_boost = CatBoostClassifier(\n",
    "        random_seed=seed,\n",
    "        eval_metric='Logloss',\n",
    "        logging_level='Silent',\n",
    "        learning_rate=0.05,\n",
    "        iterations=200\n",
    "    )\n",
    "    \n",
    "#     # 사용하지 않는 것으로 보임\n",
    "#     cat_boost_t = CatBoostClassifier(\n",
    "#         random_seed=seed,\n",
    "#         eval_metric='Logloss',\n",
    "#         logging_level='Silent',\n",
    "#         learning_rate=0.05,\n",
    "#         iterations=200\n",
    "#     )\n",
    "\n",
    "#     # 사용하지 않는 것으로 보임\n",
    "#     xgbm = XGBClassifier(\n",
    "#         objective='binary:logistic',\n",
    "#         random_state=seed,\n",
    "#         learning_rate=0.1,\n",
    "#         n_estimators=100,\n",
    "#         max_depth=8, \n",
    "#         #tree_method='gpu_hist'\n",
    "#     )\n",
    "    \n",
    "#     # 사용하지 않는 것으로 보임\n",
    "#     xgbm_t = XGBClassifier(\n",
    "#         objective='binary:logistic',\n",
    "#         random_state=seed,\n",
    "#         learning_rate=0.1,\n",
    "#         n_estimators=100,\n",
    "#         max_depth=8, \n",
    "#         #tree_method='gpu_hist'\n",
    "#     )\n",
    "    \n",
    "    models = [lgbm, cat_boost]\n",
    "    weights=[0.2, 0.8]\n",
    "    Y_validations, ensemble_val_preds, ensemble_test_preds, scores = [],[],[],[]\n",
    "    \n",
    "    # Kfold.split()으로 반환된 인덱스를 이용, 학습용 검증용 데이터 구성\n",
    "    for fold, (train_idx, val_idx) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        if debug:\n",
    "            print(f'\\nFold {fold+1}')\n",
    "            \n",
    "        X_fold_val, Y_fold_val = X.iloc[val_idx, :], y.label[val_idx]\n",
    "        X_fold_train, Y_fold_train = X.iloc[train_idx, :], y.label[train_idx]\n",
    "        \n",
    "        if debug:\n",
    "            print(f'Train shape: {X_fold_train.shape}, {Y_fold_train.shape}, Valid shape: {X_fold_val.shape}, {Y_fold_val.shape}')\n",
    "\n",
    "        # ensemble class 생성\n",
    "        ensemble_model = EnsembleHybrid(models=models, weights=weights)\n",
    "        \n",
    "        # model fit\n",
    "        ensemble_model.fit(X_fold_train, Y_fold_train)\n",
    "        \n",
    "        # model predict\n",
    "        model_prob = ensemble_model.predict_proba(X_fold_val)        \n",
    "\n",
    "        ensemble_prob = ensemble_model.predict_proba(X_fold_val)\n",
    "        \n",
    "        Y_validations.append(Y_fold_val)\n",
    "        \n",
    "        ensemble_val_preds.append(ensemble_prob)\n",
    "        \n",
    "        ensemble_test_preds.append(ensemble_model.predict_proba(X_test))        \n",
    "\n",
    "        score = log_loss(Y_fold_val, ensemble_prob)\n",
    "        scores.append(score)\n",
    "        \n",
    "        if debug:\n",
    "            print(f'Fold {fold+1} Validation Score = {score:.4f}')\n",
    "\n",
    "        del X_fold_train, Y_fold_train, X_fold_val, Y_fold_val \n",
    "        \n",
    "    mix_score = sum(scores)/FOLDS\n",
    "    \n",
    "    if debug:\n",
    "        print('Total Score (Mixing Folds Predictions) = {:.4f}'.format(mix_score))\n",
    "        \n",
    "    return mix_score, ensemble_test_preds\n",
    "\n",
    "score, ensemble_test_preds = folds_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15c3d00d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemble_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20b9e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_blend = np.zeros(X_test.shape[0])\n",
    "\n",
    "for j in range(FOLDS):\n",
    "    folds_blend += ensemble_test_preds[j]\n",
    "    \n",
    "folds_blend = folds_blend / FOLDS\n",
    "submission['pred'] = folds_blend\n",
    "submission.to_csv('final_submission_folds_mix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17eca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3932bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv(BASE_DIR / 'shc/final_submission_folds_mix.csv')\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
